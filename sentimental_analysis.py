# -*- coding: utf-8 -*-
"""sentimental analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dM-TH7Z7dQ2CgcLsrwGAi_6zrkcpj4s_
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report,accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
import numpy as np

data = pd.read_csv('/content/movie.csv')

data.shape

data = data.iloc[:,-2:]

data.head()

from collections import Counter
clas_count = Counter(data['tag'])
for i,j in clas_count.items():
  print('{}:{}'.format(i,j))

target = data.iloc[0:,-1]
target = pd.DataFrame(data = target,columns=['tag'])

target[target['tag'] == 'pos'] = 1
target[target['tag'] == 'neg'] = 0
target.head()

x = pd.DataFrame(data = data.iloc[:,0],columns = ['text'])
x.head()

def stop_words():
  words = ['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425', '10','yet','until','upon',"through", "throughout",
"thru", "thus", "to", "together", "too", "toward", "towards",
"under", '100',"here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however",
 "i", "ie", "if", "in", "indeed", "is", "it", "its", "itself", "keep", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", 
 "might", "mine", "more", "moreover", "most", "mostly", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless",
  "next","no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", 
  "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own", "part","perhaps", "please", "put", "rather", "re", "same", 
  "see", "seem", "seemed", "seeming", "seems", "she", "should","since", "sincere","so", "some", "somehow", "someone", "something", "sometime", "sometimes",
   "somewhere", "still", "such", "take","than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby","up", "upon", "us",
"very", "was", "we", "well", "were", "what", "whatever", "when","whence", "whenever", "where", "whereafter", "whereas", "whereby",
"wherein", "whereupon", "wherever", "whether", "which", "while","who", "whoever", "whom", "whose", "why", "therefore", "therein","behind", "being", "beside", "besides", "between", "beyond", "both", "thereupon", "these", 
"they", '1000','add','addition' ,'additional','can','go','h20','give','gave','given','goes','women','woman','word','got','get','buy','had','has','hadn','hasn',
'for','would','dah','wouldn','could','couldn','shall','at','by','though','again','against','been','be','before','become','becomes','because','became','as','an',
'and','all','alike','almost','also','along','always','any','anybody','asked','ask','after','can','with','without','may','might','although','despite','should',
'shouldn','will','would','10000','about','behind','beside','between', '100m', '101','yet','you','your','yours','102', '103', '104', '105', '106',
 '107', '108', '109', '10b', '10s', '10th', '11', '110', '111', '112', '113', '1138', '114', '115', '117', '118', '11th', '12', '121', '122', '123', 
 '125', '126', '127', '1272', '128', '129', '1298', '12th', '13', '130', '1305', '131', '132', '133', '135', '137', '138', '139', '13th', '14', '140', 
 '1400', '143', '144', '14th', '15', '150', '1500s', '150th', '151', '152', '1521', '153', '155', '1554', '157', '1583', '1590', '15th', '16', '160', 
 '1600', '1600s', '161', '165', '167', '1692', '16mm', '16th', '16x9', '17', '170', '1700s', '1709', '172', '175', '1773', '1791', '1792', '1793', '1794', 
 '1799', '17th', '18', '180', '1800', '1800s', '1812', '1830s', '1839', '1847', '1862', '1865', '1869', '1871', '1885', '1888', '189', '1896', '1898', '1899', 
 '18s', '18th', '19', '1900', '1900s', '1903', '1908', '1912', '1913', '1914', '1916', '1919', '1920s', '1922', '1923', '1925', '1926', '1928', '1930', '1930s', '1932',
  '1933', '1934', '1935', '1937', '1938', '1939', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1950', '1950s', '1951', '1952', 
  '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1970s', '1971', 
  '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1980s', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', 
  '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1998s', '1999', '19th', '1hr', '1st', '20', '200', '2000', '2001', '2002', '2007', '2010', '2013', '2015', '2017', '2018', '2020', '2023', '2024', '2029', '2036', '2040', '2050', '2056', '2058', '206', '2065', '209', '2099', '20s', '20somethings', '20th', '20thcentury', '21', '216', '2176', '21a', '21st', '22', '2259', '2293', '23', '230', '234', '23rd', '24', '2400', '2470', '24th', '25', '250', '254', '25th', '26', '2654', '26min', '26th', '27', '28', '280', '289', '28th', '28up', '29', '2_', '2am', '2d', '2hr', '2nd', '2th', '30', '300', '3000', '30ish', '30m', '30s', '30th', '31', '310', '31st', '32', '33', '34', '3411', '3465', '34th', '35', '357', '35mm', '35th', '36', '360', '3654', '36th', '37', '37th', '38th', '39', '3d', '3p0', '3po', '3rd', '40', '400', '40mins', '40s', '41', '42', '426', '43', '44', '449', '45', '460', '47', '48', '48th', '49', '4960', '4am', '4th', '50', '500', '5000', '50000', '500k', '500th', '50s', '50th', '51', '51st', '52', '53', '54', '54th', '55', '555', '56', '5671', '56k', '57', '571', '57th', '58', '59', '5th', '60', '600', '6000', '607', '60s', '61', '62', '63', '64', '640', '65', '65th', '66', '666', '67', '68', '69', '6th', '70', '700', '7000', '701', '70ies', '70m', '70mm', '70s', '710', '712', '73', '747s', '75', '750', '76', '77', '777', '779', '78', '79', '7th', '80', '800', '802', '8034', '80s', '81', '82', '8216', '83', '84', '85', '85but', '86', '87', '88', '89', '8a', '8mm', '8th', '90', '900', '90210', '90s', '91', '911', '92', '92ll', '92re', '92s', '92t', '92ve', '93', '939', '94', '95', '95s', '96', '97', '98', '99', '999', '9mm', '_2001_', '_21_jump_street_', '_48_hrs', '_54_', '___', '____', '_____', '______', '____________________________________________', '__________________________________________________________', '_a','_all_', '_am_','_and_','_are_','_but', '_can', '_can_', '_h20_','_have_','_his_','_here_','_in', '_into_', '_is_', '_it','_so','_that_', '_the', '_the_','_their_', '_there_', '_they', '_this_',  '_to','_too_','aa', 'aaa', 'aaaaaaaaah', 'aaaaaaaahhhh', 'aaaaaah', 'aaaahhhs', 'aahs','are','beacuse','he','she','her','him','but','zzzzzzz']
  return  words

tran = TfidfVectorizer(stop_words = stop_words(),analyzer='word',smooth_idf=False)
fit = tran.fit_transform(x['text'])
head = tran.get_feature_names()

text_data = pd.DataFrame(data = fit.toarray(),columns=head)
text_data.head()

tran.get_feature_names()

x_train,x_test,y_train,y_test = train_test_split(text_data,target,train_size=0.75,random_state=1)
print(x_train.shape,x_test.shape)
y_train = y_train.astype('int')
y_test = y_test.astype('int')

model = MultinomialNB()

model.fit(x_train,y_train.values.ravel())
y_pred = model.predict(x_test)

score = accuracy_score(y_test,y_pred)
classification = classification_report(y_test,y_pred)
print("score:{}".format(score))
print("report:{}".format(classification))

